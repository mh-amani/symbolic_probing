# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: pvr
  - override /model: transformer_dbn_classifier # transformer_dbn_classifier, gpt2
  - override /callbacks: default
  - override /trainer: default

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters
name: "pvr"
# run_name: "${model.key}-${model.discrete_layer.key}" 
run_name: "dbn transformer"

tags: []

seed: 42

trainer:
  min_epochs: 200
  max_epochs: 200
  # gradient_clip_val: 0.5 # TODO: add gradient clipping
  check_val_every_n_epoch: 1
  accelerator: "gpu"

model:
  optimizer:
    lr: 0.00001
  
  
data:
  batch_size: 256

logger:
  wandb:
    tags: ${tags}
    # group: "mnist"