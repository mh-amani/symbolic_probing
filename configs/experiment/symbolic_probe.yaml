# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: read_activation_from_file
  - override /model: encdec_enc
  - override /callbacks: default
  - override /trainer: gpu

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters
name: "symbolic_probing" 
run_name: "debugging"
tags: []
test: false
seed: 42

trainer:
  min_epochs: 1000
  max_epochs: 1000
  overfit_batches: 1
  check_val_every_n_epoch: 1
  # max_steps: 
  # gradient_clip_val: 0.5 # TODO: add gradient clipping
  # accelerator: "gpu"

model:
  optimizer:
    lr: 0.00001
  monitor: "train/loss"

callbacks:
  model_checkpoint:
    monitor: "train/loss"
    mode: "min"

  early_stopping:
    monitor: "train/loss"
  
data:
  batch_size: 32

logger:
  wandb:
    tags: ${tags}
    # group: "mnist"