_target_: src.models.encdec_enc.EncDecEncModel
encdec_config:
  vocab_size: 50265
  max_position_embeddings: 1024
  encoder_layers: 12
  encoder_ffn_dim: 4096
  encoder_attention_heads: 16
  decoder_layers: 12
  decoder_ffn_dim: 4096
  decoder_attention_heads: 16
  d_model: 1024
  use_cache: true
enc_config:
  vocab_size: 30522
  hidden_size: 768
  num_hidden_layers: 4
  num_attention_heads: 5
  intermediate_size: 3072
  max_position_embeddings: 256
  type_vocab_size: 1
  use_cache: true
symbolic_discretizer:
  _target_: blocks.modules.discrete_bottleneck.softmax.SoftmaxDiscreteBottleneck
  config:
    dimensions:
      decoder_embedding_dim: 1024
      vocab_size: 20
      encoder_embedding_dim: 1024
      unembedding_dim': 20
    quantize_vector: true
    temperature: 1.0
    encoder_embedding_trainable: true
    decoder_embedding_trainable: true
    linear_head_trainable: true
optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.95
  patience: 10
  cooldown: 0
monitor: val/loss
