_target_: src.models.encdec_enc.EncDecEncModel
encdec_config:
  _target_: transformers.BartConfig
  vocab_size: 50265
  max_position_embeddings: 256
  encoder_layers: 4
  encoder_ffn_dim: 4096
  encoder_attention_heads: 4
  decoder_layers: 4
  decoder_ffn_dim: 4096
  decoder_attention_heads: 4
  d_model: 1024
  use_cache: true
enc_config:
  _target_: transformers.BertConfig
  vocab_size: 20
  hidden_size: 1024
  num_hidden_layers: 4
  num_attention_heads: 4
  intermediate_size: 3072
  max_position_embeddings: 256
  type_vocab_size: 1
  use_cache: true
probe_discretizer:
  _target_: blocks.modules.discrete_bottleneck.softmax.SoftmaxDiscreteBottleneck
probe_discretizer_config:
  dimensions:
    decoder_embedding_dim: 1024
    vocab_size: $enc_config.vocab_size
    encoder_embedding_dim: 1024
    unembedding_dim: $enc_config.vocab_size
  quantize_vector: true
  temperature: 1.0
  encoder_embedding_trainable: true
  decoder_embedding_trainable: true
  linear_head_trainable: true
input_discretizer:
  _target_: blocks.modules.discrete_bottleneck.abstract_discrete_layer.AbstractDiscreteLayer
input_discretizer_config:
  dimensions:
    decoder_embedding_dim: 1024
    vocab_size: $encdec_config.vocab_size
    encoder_embedding_dim: 1024
    unembedding_dim: $encdec_config.vocab_size
  quantize_vector: true
  temperature: 1.0
  encoder_embedding_trainable: true
  decoder_embedding_trainable: true
  linear_head_trainable: true
optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  lr: 0.001
  weight_decay: 0.0
scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.95
  patience: 10
  cooldown: 0
monitor: val/loss
